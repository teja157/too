{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S7jRPSnypZhI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (10.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eswar teja\\onedrive\\desktop\\teja\\teja\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint,Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zfrBce3fZic-",
    "outputId": "eb9547c5-0dff-4873-cd1c-28d9045d0b98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Eswar Teja\\\\OneDrive\\\\Desktop\\\\teja\\\\teja'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41KZq4kVCNAq",
    "outputId": "9bf78788-0c6a-417c-c810-555f72fe295d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "unzip:  cannot find either lol_dataset.zip or lol_dataset.zip.zip.\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n",
    "!unzip -q lol_dataset.zip && rm lol_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97xa3JOjEkoe",
    "outputId": "71a549f3-a808-4d81-8301-ed352da8c212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: (TensorSpec(shape=(2, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(2, 128, 128, 3), dtype=tf.float32, name=None))\n",
      "Val Dataset: (TensorSpec(shape=(2, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(2, 128, 128, 3), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 2\n",
    "MAX_TRAIN_IMAGES = 300\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image.set_shape([None, None, 3])\n",
    "    image = tf.cast(image, dtype=tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_crop(low_image, enhanced_image):\n",
    "    low_image_shape = tf.shape(low_image)[:2]\n",
    "    low_w = tf.random.uniform(\n",
    "        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n",
    "    )\n",
    "    low_h = tf.random.uniform(\n",
    "        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n",
    "    )\n",
    "    low_image_cropped = low_image[\n",
    "        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n",
    "    ]\n",
    "    enhanced_image_cropped = enhanced_image[\n",
    "        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n",
    "    ]\n",
    "    # in order to avoid `NONE` during shape inference\n",
    "    low_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    enhanced_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    return low_image_cropped, enhanced_image_cropped\n",
    "\n",
    "\n",
    "def load_data(low_light_image_path, enhanced_image_path):\n",
    "    low_light_image = read_image(low_light_image_path)\n",
    "    enhanced_image = read_image(enhanced_image_path)\n",
    "    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n",
    "    return low_light_image, enhanced_image\n",
    "\n",
    "\n",
    "def get_dataset(low_light_images, enhanced_images):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n",
    "train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n",
    "\n",
    "val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n",
    "val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n",
    "\n",
    "test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n",
    "test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))\n",
    "\n",
    "\n",
    "train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n",
    "val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n",
    "\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset.element_spec)\n",
    "print(\"Val Dataset:\", val_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NZZaXiVmFUjw"
   },
   "outputs": [],
   "source": [
    "\n",
    "def selective_kernel_feature_fusion(\n",
    "    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n",
    "):\n",
    "    channels = list(multi_scale_feature_1.shape)[-1]\n",
    "    combined_feature = layers.Add()(\n",
    "        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n",
    "    )\n",
    "    gap = layers.GlobalAveragePooling2D()(combined_feature)\n",
    "    channel_wise_statistics = layers.Reshape((1, 1, channels))(gap)\n",
    "    compact_feature_representation = layers.Conv2D(\n",
    "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
    "    )(channel_wise_statistics)\n",
    "    feature_descriptor_1 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_descriptor_2 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_descriptor_3 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n",
    "    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n",
    "    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n",
    "    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n",
    "    return aggregated_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QTn3c2R6Fmjy"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ChannelPooling(layers.Layer):\n",
    "    def __init__(self, axis=-1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.axis = axis\n",
    "        self.concat = layers.Concatenate(axis=self.axis)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        average_pooling = tf.expand_dims(tf.reduce_mean(inputs, axis=-1), axis=-1)\n",
    "        max_pooling = tf.expand_dims(tf.reduce_max(inputs, axis=-1), axis=-1)\n",
    "        return self.concat([average_pooling, max_pooling])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"axis\": self.axis})\n",
    "\n",
    "\n",
    "def spatial_attention_block(input_tensor):\n",
    "    compressed_feature_map = ChannelPooling(axis=-1)(input_tensor)\n",
    "    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(compressed_feature_map)\n",
    "    feature_map = keras.activations.sigmoid(feature_map)\n",
    "    return input_tensor * feature_map\n",
    "\n",
    "\n",
    "def channel_attention_block(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "    feature_descriptor = layers.Reshape((1, 1, channels))(average_pooling)\n",
    "    feature_activations = layers.Conv2D(\n",
    "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
    "    )(feature_descriptor)\n",
    "    feature_activations = layers.Conv2D(\n",
    "        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n",
    "    )(feature_activations)\n",
    "    return input_tensor * feature_activations\n",
    "\n",
    "\n",
    "def dual_attention_unit_block(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    feature_map = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(input_tensor)\n",
    "    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n",
    "        feature_map\n",
    "    )\n",
    "    channel_attention = channel_attention_block(feature_map)\n",
    "    spatial_attention = spatial_attention_block(feature_map)\n",
    "    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n",
    "    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n",
    "    return layers.Add()([input_tensor, concatenation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7J7SXtSrFz-7"
   },
   "outputs": [],
   "source": [
    "# Recursive Residual Modules\n",
    "\n",
    "\n",
    "def down_sampling_module(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
    "        input_tensor\n",
    "    )\n",
    "    main_branch = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(main_branch)\n",
    "    main_branch = layers.MaxPooling2D()(main_branch)\n",
    "    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n",
    "    skip_branch = layers.MaxPooling2D()(input_tensor)\n",
    "    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n",
    "    return layers.Add()([skip_branch, main_branch])\n",
    "\n",
    "\n",
    "def up_sampling_module(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
    "        input_tensor\n",
    "    )\n",
    "    main_branch = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(main_branch)\n",
    "    main_branch = layers.UpSampling2D()(main_branch)\n",
    "    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n",
    "    skip_branch = layers.UpSampling2D()(input_tensor)\n",
    "    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n",
    "    return layers.Add()([skip_branch, main_branch])\n",
    "\n",
    "\n",
    "# MRB Block\n",
    "def multi_scale_residual_block(input_tensor, channels):\n",
    "    # features\n",
    "    level1 = input_tensor\n",
    "    level2 = down_sampling_module(input_tensor)\n",
    "    level3 = down_sampling_module(level2)\n",
    "    # DAU\n",
    "    level1_dau = dual_attention_unit_block(level1)\n",
    "    level2_dau = dual_attention_unit_block(level2)\n",
    "    level3_dau = dual_attention_unit_block(level3)\n",
    "    # SKFF\n",
    "    level1_skff = selective_kernel_feature_fusion(\n",
    "        level1_dau,\n",
    "        up_sampling_module(level2_dau),\n",
    "        up_sampling_module(up_sampling_module(level3_dau)),\n",
    "    )\n",
    "    level2_skff = selective_kernel_feature_fusion(\n",
    "        down_sampling_module(level1_dau),\n",
    "        level2_dau,\n",
    "        up_sampling_module(level3_dau),\n",
    "    )\n",
    "    level3_skff = selective_kernel_feature_fusion(\n",
    "        down_sampling_module(down_sampling_module(level1_dau)),\n",
    "        down_sampling_module(level2_dau),\n",
    "        level3_dau,\n",
    "    )\n",
    "    # DAU 2\n",
    "    level1_dau_2 = dual_attention_unit_block(level1_skff)\n",
    "    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n",
    "    level3_dau_2 = up_sampling_module(\n",
    "        up_sampling_module(dual_attention_unit_block(level3_skff))\n",
    "    )\n",
    "    # SKFF 2\n",
    "    skff_ = selective_kernel_feature_fusion(level1_dau_2, level2_dau_2, level3_dau_2)\n",
    "    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n",
    "    return layers.Add()([input_tensor, conv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3fTrUzDaGGJ3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Eswar Teja\\OneDrive\\Desktop\\teja\\teja\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Eswar Teja\\OneDrive\\Desktop\\teja\\teja\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def recursive_residual_group(input_tensor, num_mrb, channels):\n",
    "    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
    "    for _ in range(num_mrb):\n",
    "        conv1 = multi_scale_residual_block(conv1, channels)\n",
    "    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n",
    "    return layers.Add()([conv2, input_tensor])\n",
    "\n",
    "\n",
    "def mirnet_model(num_rrg, num_mrb, channels):\n",
    "    input_tensor = keras.Input(shape=[None, None, 3])\n",
    "    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
    "    for _ in range(num_rrg):\n",
    "        x1 = recursive_residual_group(x1, num_mrb, channels)\n",
    "    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n",
    "    output_tensor = layers.Add()([input_tensor, conv])\n",
    "    return keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "\n",
    "model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "A-ZUZA_6GKmu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5p5I00OrGO4I",
    "outputId": "a79f4508-1aa1-4f6d-ab77-9b75004e5fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\Eswar Teja\\OneDrive\\Desktop\\teja\\teja\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Eswar Teja\\OneDrive\\Desktop\\teja\\teja\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " 10/150 [=>............................] - ETA: 2:04:13 - loss: 0.2750 - peak_signal_noise_ratio: 59.3638"
     ]
    }
   ],
   "source": [
    "# Your custom loss and metric functions\n",
    "def charbonnier_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n",
    "\n",
    "def peak_signal_noise_ratio(y_true, y_pred):\n",
    "    return tf.image.psnr(y_pred, y_true, max_val=255.0)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=charbonnier_loss,\n",
    "    metrics=[peak_signal_noise_ratio],\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "k1 = ReduceLROnPlateau(\n",
    "    monitor=\"val_peak_signal_noise_ratio\",\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_delta=1e-7,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# k2 = ModelCheckpoint(\n",
    "#     \"LLIE.h5\",\n",
    "#     monitor=\"val_peak_signal_noise_ratio\",\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=False,  # Set to True if you only want to save model weights\n",
    "#     mode=\"max\",\n",
    "#     verbose=1,\n",
    "# )\n",
    "k2 = ModelCheckpoint( filepath=\"LLIE.h5\",  monitor=\"val_peak_signal_noise_ratio\", verbose=1, save_best_only=True, save_weights_only=True, mode=\"auto\", save_freq=\"epoch\" )\n",
    "# Train the model and capture the history\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=2,\n",
    "    callbacks=[k1,k2]\n",
    ")\n",
    "\n",
    "# Check if history is not None\n",
    "if history is not None:\n",
    "    # Your plot_history function\n",
    "    def plot_history(value, name):\n",
    "        plt.plot(history.history[value], label=f\"train_{name.lower()}\")\n",
    "        plt.plot(history.history[f\"val_{value}\"], label=f\"val_{name.lower()}\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(name)\n",
    "        plt.title(f\"Train and Validation {name} Over Epochs\", fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    plot_history(\"loss\", \"Loss\")\n",
    "    plot_history(\"peak_signal_noise_ratio\", \"PSNR\")\n",
    "else:\n",
    "    print(\"Training failed. Check your data and model configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-6wge8DYoCs"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(images, titles, figure_size=(12, 12)):\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    for i in range(len(images)):\n",
    "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
    "        _ = plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def infer(original_image):\n",
    "    image = keras.utils.img_to_array(original_image)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    output = model.predict(image, verbose=0)\n",
    "    output_image = output[0] * 255.0\n",
    "    output_image = output_image.clip(0, 255)\n",
    "    output_image = output_image.reshape(\n",
    "        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n",
    "    )\n",
    "    output_image = Image.fromarray(np.uint8(output_image))\n",
    "    original_image = Image.fromarray(np.uint8(original_image))\n",
    "    return output_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Q1vJ0HTMYw8P",
    "outputId": "9ad8a9b4-fe2d-4dde-f7e7-3e6d606aba48"
   },
   "outputs": [],
   "source": [
    "\n",
    "for low_light_image in random.sample(test_low_light_images, 6):\n",
    "    original_image = Image.open(low_light_image)\n",
    "    enhanced_image = infer(original_image)\n",
    "    plot_results(\n",
    "        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n",
    "        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n",
    "        (20, 12),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clw8ti_pZA1G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
